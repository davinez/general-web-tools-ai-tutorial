# Notes

AI solutions are built on machine learning models that encapsulate semantic relationships found in huge quantities of data; enabling applications to appear to interpret input in various formats, reason over the input data, and generate appropriate responses and predictions.

* Table Common AI capabilities that developers can integrate into a software application include:
https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/2-what-is-ai


# Type of AI 

## Generative AI

- Generative AI uses language models to respond to natural language prompts, enabling you to build conversational apps and agents that support research, content creation, and task automation 

- The language models used in generative AI solutions can be large language models (LLMs) that have been trained on huge volumes of data and include many millions of parameters; or they can be small language models (SLMs) that are optimized for specific scenarios with lower overhead. 

- Language models commonly respond to text-based prompts with natural language text; though increasingly new multi-modal models are able to handle image or speech prompts and respond by generating text, code, speech, or images.

- Generative AI applications are built on language models. The development process usually starts with an exploration and comparison of available foundation models to find the one that best suits the particular needs of your application. After selecting a suitable model, you deploy it to an endpoint where it can be consumed by a client application or AI agent.

Foundation models, such as the GPT family of models, are state-of-the-art language models designed to understand, generate, and interact with natural language. Some common use cases for models are:

-- Speech-to-text and text-to-speech conversion. For example, generate subtitles for videos.
-- Machine translation. For example, translate text from English to Japanese.
-- Text classification. For example, label an email as spam or not spam.
-- Entity extraction. For example, extract keywords or names from a document.
-- Text summarization. For example, generate a short one-paragraph summary from a multi-page document.
-- Question answering. For example, provide answers to questions like "What is the capital of France?"
-- Reasoning. For example, solve a mathematical problem.


* IMPORTANT:
The latest breakthrough in generative AI models is owed to the development of the Transformer architecture:

- Instead of processing words sequentially, Transformers process 
each word independently and in parallel by using attention.
- Next to the semantic similarity between words, 
Transformers use positional encoding to include the information about the position of a word in a sentence.




# List Azure AI services

A set of out-of-the-box prebuilt APIs and models that you can integrate into your applications. The following table lists some commonly used Azure AI services

* Table of services 
https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/3-azure-ai-services

* Complete list of services
https://learn.microsoft.com/en-us/azure/ai-services/what-are-ai-services#available-azure-ai-services?azure-portal=true


# How provisioned / create azure services ?

- Portal
- BICEP or ARM templates
- Azure command-line interface
- For most medium to large-scale development scenarios it's better 
use concentrate in an Azure AI Foundry project 
enabling centralize access control and cost management

# Regional availability

Some services and models are available in only a subset of Azure regions.


# Single service or multi-service resource ?

## Single Service

- Most Azure AI services, such as Azure AI Vision, Azure AI Language, and so on, 
can be provisioned as standalone resources.

- Standalone Azure AI services often include a free-tier SKU with limited functionality

- Each standalone Azure AI resource provides an endpoint and authorization keys to access it

## Multi-service

- Encapsulates multiple AI services in a single Azure resource

- We have 2 options types for multi-service: Azure AI services and Azure AI Foundry

![alt text](./images-cert/image.png)


# Azure AI services

- Azure AI services uses a single key and endpoint

# Azure AI Foundry

- Resource connections, data, code, and other elements of the AI solution in projects. (centralized)
- Azure AI Foundry portal, a web-based visual interface for working with AI projects. 
- Azure AI Foundry SDK, which you can use to build AI solutions programmatically


## Types of AI Foundry projects

- Foundry projects

-- Associated with an Azure AI Foundry resource in an Azure subscription. 
-- Foundry projects provide support for Azure AI Foundry models (including OpenAI models), 
Azure AI Foundry Agent Service, Azure AI services, and tools for evaluation 
and responsible AI development
-- More suitable for developers who want to manage resources for AI agent 
or chat app development. 


- Hub-based projects

-- Associated with an Azure AI hub resource.
-- Hub-based projects include an Azure AI Foundry resource, 
as well as managed compute, support for Prompt Flow development, and connected 
Azure storage and Azure key vault resources
-- We can also use Azure AI hub resources in both Azure AI Foundry portal 
and Azure Machine learning portal, making it easier to work on collaborative 
projects that involve data scientists and machine learning specialists as 
well as developers and AI software engineers
-- More suitable for enterprise development teams working on complex AI solutions.

# Developer tools and SDKs

- Azure AI Foundry for VS Code extension:
Creating a project.
Selecting and deploying a model.
Testing a model in the playground.
Creating an agent.

- GitHub and GitHub Copilot

- The Azure AI Foundry Models API, which provides an interface for 
working with generative AI model endpoints hosted in Azure AI Foundry.

- The Azure OpenAI in Azure AI Foundry Models API, 
which enables you to build chat applications based on OpenAI models hosted 
in Azure AI Foundry.
* Using API POST like GEMINI for .NET, if there is no proper SDK

* Difference 
* The Model Inference API primarily concerns the consumption of predictions 
from a wide range of foundational models.
* The Foundry Models API focuses on AI agents and also includes Azure OpenAI 
and other compatible models for agent-based applications.

- Azure AI Services SDKs, AI service-specific libraries for 
multiple programming languages and frameworks that enable you to consume 
Azure AI Services resources in your subscription. 

- The Azure AI Foundry SDK, which enables you to write code to connect to 
Azure AI Foundry projects and access resource connections, 
which you can then work with using service-specific SDKs.

- The Azure AI Foundry Agent Service, which is accessed through the 
Azure AI Foundry SDK and can be integrated with frameworks 
like *Semantic Kernel* to build comprehensive AI agent solutions.


# Responsible AI

Probabilistic models, which are in turn dependent on the data with 
which they were trained.

- Fairness, should treat all people fairly. Example, 
suppose you create a machine learning model to support a loan approval 
application for a bank. The model should make predictions of whether or not 
the loan should be approved without incorporating any bias based on gender, ethnicity, 
or other factors that might result in an unfair advantage or disadvantage to specific groups of applicants.

- Reliability and safety, a machine learning model that diagnoses 
patient symptoms and recommends prescriptions. 
Unreliability in these kinds of system can result in substantial 
risk to human life.


- Privacy and security
The machine learning models on which AI systems are based rely on large volumes 
of data, which may contain personal details that must be kept private. 
Even after models are trained and the system is in production, 
they use new data to make predictions or take action that may be subject 
to privacy or security concerns; so appropriate safeguards to protect data 
and customer content must be implemented.

- Inclusiveness
AI systems should empower everyone and engage people. 
AI should bring benefits to all parts of society, 
regardless of physical ability, gender, sexual orientation, ethnicity, 
or other factors.

- Transparency
AI systems should be understandable. Users should be made fully 
aware of the purpose of the system, 
how it works, and what limitations may be expected.

- Accountability
Designers and developers of AI-based solution should work within a framework 
of governance and organizational principles that ensure the solution 
meets responsible and legal standards that are clearly defined.


# Choose and deploy models from the model catalog in Azure AI Foundry portal

# Selecting a foundation model for your generative AI app 

## Can AI solve my use case?

To start answering this question, you need to discover, filter, and deploy a model. 
We can explore the available language models through three different catalogs:

- Hugging Face: Vast catalog of open-source models across various domains.
- GitHub: Access to diverse models via GitHub Marketplace and GitHub Copilot.
- Azure AI Foundry: Comprehensive catalog with robust tools for deployment.

### Choose between large and small language models

- Large Language Models (LLMs) like GPT-4, Mistral Large, Llama3 70B, 
Llama 405B, and Command R+ are powerful AI models 
designed for tasks that require deep reasoning, 
complex content generation, and extensive context understanding.

- Small Language Models (SLMs) like Phi3, Mistral OSS models, and Llama3 8B 
are efficient and cost-effective, while still handling many common 
Natural Language Processing (NLP) tasks. They're perfect for running on 
lower-end hardware or edge devices, where cost and speed are more important than model complexity.


### Focus on a modality, task, or tool

- *chat completion models*: Language models like GPT-4 and Mistral Large are 
also known as designed to generate coherent and contextually appropriate text-based responses.

- *reasoning models*: Tasks like math, coding, science, strategy, and logistics, 
you can use like DeepSeek-R1 and o1.

- *multi-modal*: meaning they can process images, audio, and other data 
types alongside text. Models like GPT-4o and Phi3-vision are capable of analyzing and generating both text and images.

- *generating images*: tools like DALL·E 3 and Stability AI can create realistic visuals from text prompts.

- *embedding models*: like Ada and Cohere. Embeddings models convert 
text into numerical representations and are used to improve search 
relevance by understanding semantic meaning. 
These models are often implemented in *Retrieval Augmented Generation (RAG)* 
scenarios to enhance recommendation engines by linking similar content.

* When you want to build an application that interacts with other software 
tools dynamically, you can add function calling and JSON support. 


### Specialize with regional and domain-specific models

Certain models are designed for specific languages, regions, or industries. 
These models can outperform general-purpose generative AI in their respective domains. 

For example:

- Core42 JAIS is an Arabic language LLM, making it the best 
choice for applications targeting Arabic-speaking users.

- Mistral Large has a strong focus on European languages, 
ensuring better linguistic accuracy for multilingual applications.

- Nixtla TimeGEN-1 specializes in time-series forecasting, making it ideal for financial predictions, supply chain optimization, and demand forecasting.

* If your project has regional, linguistic, or industry-specific needs, these models can provide more relevant results than general-purpose AI.

### Balance flexibility and performance with open versus proprietary models

- *Proprietary models* are best for cutting-edge performance and enterprise use. 
Azure offers models like OpenAI’s GPT-4, Mistral Large, and Cohere Command R+, 
which deliver industry-leading AI capabilities. 
These models are ideal for businesses needing enterprise-level security, support, and high accuracy.

- *Open-source models* are best for flexibility and cost-efficiency. 
Many open-source models available in the Azure AI Foundry model catalog 
from Hugging Face, and models from Meta, Databricks, Snowflake, and Nvidia. 

* All the models available in Azure AI Foundry meets the key enterprise requirements for usage:
- Data and privacy: you get to decide what happens with your data.
- Security and compliance: built-in security.
- Responsible AI and content safety: evaluations and content safety.


## How do I select the best model for my use case?

- Task type: What type of task do you need the model to perform? 
Does it include the understanding of only text, or also audio, or video, or multiple modalities?

- Precision: Is the base model good enough or do you need a fine-tuned model 
that is trained on a specific skill or dataset?

- Openness: Do you want to be able to fine-tune the model yourself?

- Deployment: Do you want to deploy the model locally, 
on a serverless endpoint, or do you want to manage the deployment infrastructure?

### Filter models for precision

- When integrating a language model into an app, you can choose between a 
base model or a fine-tuned model. 
A base model, like GPT-4, is pretrained on a large dataset and can 
handle various tasks but can lack precision for specific domains. 
Techniques like prompt engineering can improve this, but sometimes fine-tuning is necessary.

- A *f*ine-tuned model* is trained further on a smaller, task-specific dataset to improve its precision and ability to generate relevant outputs for specific applications. You can either use a *fine-tuned model* or *fine-tune* a model yourself.

### Filter models for performance

* Table When you're exploring models through the Azure AI Foundry model catalog, 
you can use model benchmarks to compare publicly available metrics:

https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/2-select-model

- To evaluate how a selected model performs regarding your specific requirements, 
you can consider manual or automated evaluations.

- *Manual evaluations* allow you to rate your model's responses. 
- *Automated evaluations* include traditional machine learning metrics and AI-assisted metrics that are calculated and generated for you.


## Can I scale for real-world workloads?

Considerations for scaling a generative AI solution include:

- Model deployment: Where will you deploy the model for the best balance of performance and cost?
- Model monitoring and optimization: How will you monitor, evaluate, and optimize model performance?
- Prompt management: How will you orchestrate and optimize prompts to maximize the accuracy and relevance of generated responses?
- Model lifecycle: How will you manage model, data, and code updates as part of an ongoing Generative AI Operations (GenAIOps) lifecycle?


## Deploy a model to an endpoint

When you develop a generative AI app, you need to integrate language models into your application. 
To be able to use a language model, you need to deploy the model.

1- API request is sent to the endpoint.
2- The endpoint specifies the model that processes the request.
3- The result is sent back to the app through an API response.


When you deploy a language model with Azure AI Foundry, 
you have several types available, which depend on the model you want to deploy.

- Standard deployment: Models are hosted in the Azure AI Foundry project resource.

- Serverless compute: Models are hosted in Microsoft-managed dedicated serverless endpoints in an Azure AI Foundry hub project.

- Managed compute: Models are hosted in managed virtual machine images in an Azure AI Foundry hub project.


* The associated cost depends on the type of model you deploy, 
which deployment option you choose, and what you are doing with the model:
* Standard deployment is recommended for most scenarios.

![alt text](./images-cert/image2.png)


# Apply prompt patterns to optimize your model's output

- The process of designing and optimizing prompts to improve the model's performance 
is also known as *prompt engineering*

- An effective approach is to add instructions to your deployed model 
in the form of a system prompt. The system prompt sets the model's behavior 
and allows you to guide the model without exposing the end user to the 
instructions. 
The best results are often achieved through the assignment of an *explicit system prompt* and *guidance (or templates)*

## Specify the desired format for responses

- When you want the model to generate output in a specific format, 
you can provide a template or structure in your prompt.
Example, if you're a sports reporting composing a historical article, 
you can request that the model follow a specific template,
which includes headings, bullet points, and data breakdowns.

## Add context

- When you want the model to focus on specific topics, 
you can specify the context to consider.

No context specified:
When should I visit Edinburgh?

With context specified:
When should I visit Edinburgh?
I'm particularly interested in attending Scotland's home matches 
in the Six Nations rugby tournament.


## Apply model optimization strategies

- *Retrieval Augmented Generation (RAG)*: A technique that involves 
using a data source to provide grounding context to prompts. 
RAG can be a useful approach when you need the model to answer questions 
based on a specific knowledge domain or when you need the model to consider 
information related to events that occurred after the training data on which the model is based.

- *Fine-tuning*: A technique that involves extending the training of a 
foundation model by providing example prompts and responses that reflect 
the desired output format and style.

## The strategy you should choose as a developer depends requirements

1- Prompt engineering
2- Combined strategies
3- Retrieval Augmented Generation (RAG)
4- Fine-tuning

- Optimize for context: When the model lacks contextual knowledge 
and you want to maximize responses accuracy: we can us RAG for example

- Optimize the model: When you want to improve the response format, style, 
or speech by maximizing consistency of behavior: With fine-tuning, we train a 
base language model on a dataset of example prompts and responses before 
integrating it in your application, with the result that the fine-tuned model 
will produce responses that are consistent with the examples 
in the fine-tuning training dataset.

* We can use any combination of optimization strategies, 
for example prompt engineering, RAG and a fine-tuned model, 
to improve your language application.



# Get started with prompt flow to develop language model apps in the Azure AI Foundry

## Prompt flow

- To harness the power of the LLMs available, you need to create an application that combines your data sources with LLMs and generates the desired output.

- To develop, test, tune, and deploy LLM applications, you can use prompt flow, accessible in the *Azure Machine Learning studio* and 
the *Azure AI Foundry portal*.

### Understand the development lifecycle of a large language model (LLM) app

![alt text](./images-cert/image3.png)

1- Initialization: Define the use case and design the solution.
2- Experimentation: Develop a flow and test with a small dataset.
3- Evaluation and refinement: Assess the flow with a larger dataset.
4- Production: Deploy and monitor the flow and application.


#### Initialization

1- Define the objective
2- Collect a sample dataset
3- Build a basic prompt
4- Design the flow

#### Experimentation

- The experimentation phase is an iterative process during which you 
(1) run the flow against a sample dataset. You then 
(2) evaluate the prompt's performance. If you're 
(3) satisfied with the result, you can move on to evaluation and refinement. If you think there's room for improvement, you can 
(4) modify the flow by changing the prompt or flow itself.

#### Evaluation and refinement

- When you're satisfied with the output of the flow that classifies news articles, based on the sample dataset, you can assess the flow's performance against a larger dataset.

#### Production

During production, you:

1- Optimize the flow that classifies/process/etc
2- Deploy your flow to an endpoint. When you call the endpoint, the flow is triggered to run and the desired output is generated.
3- Monitor the performance of your solution by collecting usage data and end-user feedback. 
By understanding how the application performs, you can improve the flow whenever necessary.


## Understand prompt flow's core components

- Prompt flow is a feature within Azure AI Foundry that allows you to author flows. Flows are executable workflows often consist of three parts:

1- Inputs: Represent data passed into the flow. Can be different data types like strings, integers, or boolean.
2- Nodes: Represent tools that perform data processing, task execution, or algorithmic operations.
3- Outputs: Represent the data produced by the flow.

- Similar to a pipeline, a flow can consist of multiple nodes that 
can use the flow's inputs or any output generated by another node. 
You can add a node to a flow by choosing one of the available types of tools.

### Tools available in prompt flow

Three common tools are:

- LLM tool: Enables custom prompt creation utilizing Large Language Models.
- Python tool: Allows the execution of custom Python scripts.
- Prompt tool: Prepares prompts as strings for complex scenarios or integration with other tools.

* Note: If we're looking for functionality that is not offered by the available tools, we can create your own custom tool

By defining the inputs, connecting nodes, and defining the desired outputs, you can create a flow. Flows help you create LLM applications for various purposes.

### Understand the types of flows

There are three different types of flows you can create with prompt flow:

- Standard flow: Ideal for general LLM-based application development, offering a range of versatile tools.
- Chat flow: Designed for conversational applications, with enhanced support for chat-related functionalities.
- Evaluation flow: Focused on performance evaluation, allowing the analysis and improvement of models or 
applications through feedback on previous runs.


## Explore connections and runtimes


When we create a Large Language Model (LLM) application with prompt flow, 
we first need to configure any necessary *connections* and *runtimes*

### Connections

- Whenever we want our flow to connect to external data source, service, or API,
we need our flow to be authorized to communicate with that external service. 

When we create a connection, we configure a secure link between prompt flow and 
external services

- Depending on the type of connection we create, 
the connection securely stores the endpoint, API key, or 
credentials necessary for prompt flow to communicate with the external service. 
Any necessary secrets aren't exposed to users, but instead are stored in an *Azure Key Vault*

By setting up connections, users can easily reuse external services necessary for tools in their flows.

* Certain built-in tools require you to have a connection configured:

- Summary:

1- Connections automate API credential management
2- Connections enable secure data transfer from various sources, crucial for 
maintaining data integrity and privacy across different environments.

### Runtimes

Are a combination of a compute instance:
1- They provide necessary compute resources
2- Thet specify the necessary packages and libraries that need to be installed before being able to run the flow.

When you use runtimes, you have a controlled environment where flows can be run and validated, ensuring that everything works as intended in a stable setting.

* Note: A default environment is available for quick development and testing. 
When we require other packages to be installed, you can create a *custom environment*


## Explore variants and monitoring options


### Variants

- Prompt flow variants are versions of a tool node with distinct settings. 

- Currently, variants are only supported in the LLM tool

- Variant can represent a different prompt content or connection setting. 
Variants allow users to customize their approach for specific tasks, like, summarizing news articles.

Benefits:

- Enhance the quality of your LLM generation
- Save time and effort
- Boost productivity
- Facilitate easy comparison


### Deploy flow to an endpoint

- When we're satisfied with the performance of your flow, we can choose to deploy it to an online endpoint

### Metrics 

To understand whether your application is meeting practical needs:

- We can collect end-user feedback and assess the application's usefulness. 

- Another approach to understanding whether your application is performing well, 
is by comparing LLM predictions with expected or ground truth responses to gauge accuracy and relevance.

- Groundedness: Measures alignment of the LLM application's output with the input source or database.
- Relevance: Assesses how pertinent the LLM application's output is to the given input.
- Coherence: Evaluates the logical flow and readability of the LLM application's text.
- Fluency: Assesses the grammatical and linguistic accuracy of the LLM application's output.
- Similarity: Quantifies the contextual and semantic match between the LLM application's output and the ground truth.

Whenever our LLM application doesn't perform as expected, 
we need to revert back to experimentation to iteratively explore how to improve the flow


### Develop prompt flow

- The prompt flow tools in Azure AI Foundry create file-based assets that 
define the prompt flow in a folder in *blob storage*

* Note of why the headache of using promp flow and its configuration process in azure foundry
instead of directly use open ai model from azure foundry prpject:

It can seem confusing at first, but Prompt Flow offers significant advantages over directly connecting to a model, especially for building a chat application.

Here's a breakdown of the "why" behind using Prompt Flow in Azure AI Studio:

Azure AI Foundry Project vs. Prompt Flow: It's Not "Either/Or"
First, it's important to understand that an Azure AI Foundry project and Prompt Flow are not mutually exclusive. Think of your Azure AI Foundry project as your central workbench or studio. It's where you organize all the resources for your AI application: your models, data, compute resources, and deployments.

Prompt Flow, on the other hand, is a specialized tool within your AI Foundry project. It's a visual development tool that helps you design, build, and test the logic of your AI application, especially how it interacts with Large Language Models (LLMs) like those from OpenAI.

So, you are still using an OpenAI model within your Azure AI Foundry project, but you are using Prompt Flow to manage how you interact with that model.

Why the "Headache"? The Advantages of Prompt Flow
While a direct connection to an OpenAI model is quick for a simple proof-of-concept, it becomes difficult to manage as your application grows. The "long and confuse flow" you're seeing in the Microsoft Learn exercise is actually a structured way of building a more robust and sophisticated application. Here are the key advantages:

Visual and Organized Development 🧠: Prompt Flow gives you a visual graph of your application's logic. Each step, from processing the user's input to calling the LLM and formatting the output, is a distinct "node" in the flow. This visual representation makes it much easier to understand, debug, and modify your application's logic, especially as it becomes more complex. For a chat app, this could involve nodes for retrieving chat history, generating a contextually relevant prompt, calling the model, and then processing the model's response.

Streamlined Prompt Engineering and Tuning 🧪: A good AI application relies on well-crafted prompts. Prompt Flow is designed for this. You can easily:

Create and test prompt variants: Try out different versions of your prompts to see which one gives the best results.

Use templates: Create dynamic prompts that can incorporate variables and other inputs.

Evaluate performance: Use built-in tools to assess the quality and effectiveness of your prompts and the overall flow.

Built for Chatbots and Conversational AI 💬: The "Chat Flow" in the exercise is specifically designed for building conversational applications. It has built-in features to:

Manage chat history: It automatically handles the conversation history, which is crucial for a chatbot to have a coherent, multi-turn conversation.

Handle chat inputs and outputs: It provides a structured way to manage the back-and-forth of a conversation.

Scalability and Collaboration 🤝: For a real-world application, you'll likely be working with a team. Prompt Flow makes it easier to collaborate on and version control your application's logic. It also simplifies the process of deploying your flow as a scalable endpoint that your application can call.

Conditional Logic and Integration of Tools 🛠️: Prompt Flow allows you to add conditional logic (e.g., "if the user asks about X, then do Y"). You can also integrate other tools into your flow, such as Python scripts for custom data processing or APIs for external services.

In a Nutshell: From a Simple Call to a Sophisticated Application
Connecting directly to an OpenAI model is like making a single phone call. It's quick and easy for a one-off task.

Using Prompt Flow is like building a call center. It takes more effort to set up, but you get a system that can handle many calls, route them to the right place, keep track of conversations, and be easily managed and improved over time.

For the Microsoft Learn exercise, they are teaching you the more structured and scalable way to build a chat app. While it may seem like overkill for a simple example, these are the foundational skills you'll need to build more complex and production-ready AI applications. The initial "headache" of setting up the flow pays off significantly in terms of maintainability, scalability, and the ability to fine-tune your application's performance.

Also:

Azure AI Project (The Workspace)
The AI Project is where the hands-on development work happens. It's a self-contained environment that inherits the settings and connections from its parent Hub.

Key Functions:

Work Isolation: It provides an organized space for a specific use case. All your assets for the chat app—your prompt flows, notebooks, data, and deployed endpoints—are neatly contained within its project.

Team-Specific Environment: It allows teams to work independently without interfering with each other, while still leveraging the shared resources provided by the Hub.

Lifecycle Management: You manage the end-to-end lifecycle of a specific AI model or application (development, evaluation, deployment) within its project.

Why the Exercise Uses a Hub
The Microsoft Learn exercise guides you through this structure to teach best practices.

It starts with the Hub because in any real-world scenario, you first need that central foundation.

When you create your Prompt Flow, you are doing so within a Project.

That Project leverages the connection to the OpenAI model that was likely configured at the Hub level.

So, you aren't choosing between a Hub and a Project. The workflow is:

Create/Select an AI Hub ➡️ Create an AI Project within that Hub ➡️ Do your development work (like building a Prompt Flow) inside the Project.

This layered approach ensures that as you move from simple exercises to building complex, production-grade AI solutions, your work is secure, organized, and scalable.


# Develop a RAG-based solution with your own data using Azure AI Foundry

## Ungrounded prompts and responses


- When you use a language model to generate a response to a prompt, the only information 
that the model has to base the answer on comes from the data on which it was trained 
which is often just a large volume of uncontextualized text from the Internet or some other source.

- The result will likely be a grammatically coherent and logical response to the prompt, 
but because it isn't grounded in relevant, factual data, it's uncontextualized; and may 
in fact be inaccurate and include "invented" information. For example, 
the question "Which product should I use to do X?" might include details of a fictional product.

So:

- In contrast, you can use a data source to ground the prompt with some relevant, 
factual context. The prompt can then be submitted to a language model, 
including the grounding data, to generate a contextualized, relevant, and accurate response.

- The data source can be any repository of relevant data. 
For example, you could use data from a product catalog database to ground 
the prompt "Which product should I use to do X?" so that the response includes
relevant details of products that exist in the catalog.


## Understand how to ground your language model

- Though language models are trained on a vast amount of data, they may not have access to the knowledge you want to make available to your users. To ensure that an agent is grounded on specific data to provide accurate and domain-specific responses, you can use Retrieval Augmented Generation (RAG).

- It's a process for retrieving information that is relevant to the user's initial prompt. In general terms, the RAG pattern incorporates the following steps:

1- Retrieve grounding data based on the initial user-entered prompt.
2- Augment the prompt with grounding data.
3- Use a language model to generate a grounded response.

- We can use Azure AI Foundry to build a custom agent that uses your own 
data to ground prompts. 
Azure AI Foundry supports a range of data connections that you can use to add data to a project, 
including:

1- Azure Blob Storage
2- Azure Data Lake Storage Gen2
3- Microsoft OneLake
4- We can also upload files or folders to the storage used by your AI Foundry project.

![alt text](./images-cert/image4.png)


## Make data searchable / Azure AI Search

- When we build an agent with the Azure AI Foundry, we can use the integration 
with *Azure AI Search* to retrieve the relevant context in your chat flow.

- Azure AI Search is a retriever that you can include when building a language model application 
with prompt flow. Azure AI Search allows you to bring your own data, index your data, 
and query the index to retrieve any information you need.

### Using a vector index 

- While a text-based index will improve search efficiency, you can usually achieve a better data retrieval solution by using a vector-based index that contains embeddings that represent the text tokens in your data source.

- An embedding is a special format of data representation that a search engine can use to easily find the relevant information. More specifically, an embedding is a vector of floating-point numbers.

- For example, imagine you have two documents with the following contents:

-- "The children played joyfully in the park."
-- "Kids happily ran around the playground."

These two documents contain texts that are semantically related, even though different words are used. By creating vector embeddings for the text in the documents, the relation between the words in the text can be mathematically calculated.

Refer to https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/3-search-data

### Creating a search index

- In Azure AI Search, a search index describes how your content is organized to make it searchable.

- You can add your data to Azure AI Foundry, after which you can use Azure AI Search to create an index in the Azure AI Foundry portal using an embedding model. The index asset is stored in Azure AI Search and queried by Azure AI Foundry when used in a chat flow.

![alt text](./images-cert/image5.png)

# Searching an index

There are several ways that information can be queried in an index:

- Keyword search: Identifies relevant documents or passages based on specific keywords or terms provided as input.
- Semantic search: Retrieves documents or passages by understanding the meaning of the query and matching it with semantically related content rather than relying solely on exact keyword matches.
- Vector search: Currently, the most advanced technique is *vector search*, which creates embeddings to represent your data.
Uses mathematical representations of text (vectors) 
to find similar documents or passages based on their semantic meaning or context.
- Hybrid search: Combines any or all of the other search techniques. 
Queries are executed in parallel and are returned in a unified result set.
Hybrid search is a combination of keyword (and full text), and vector search, 
to which semantic ranking is optionally added. When you create an index that is 
compatible with hybrid search, the retrieved information is precise when exact matches 
are available (using keywords), and still relevant when only conceptually similar information can be found (using vector search).


## Create a RAG-based client application

- When you've created an Azure AI Search index for your contextual data, 
you can use it with an OpenAI model. To ground prompts with data from your index, 
the Azure OpenAI SDK supports extending the request with connection details for the index.

* Implement RAG in a prompt flow, refer docs: 
https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/4-build-copilot

* Note: refer to RAG Ai-102 endpoint in GWTAI app

## Implement RAG in a prompt flow

- After uploading data to Azure AI Foundry and creating an index on your data using the integration with Azure AI Search, you can implement the RAG pattern with Prompt Flow to build a generative AI application.

* Prompt Flow is a development framework for defining flows that orchestrate interactions with an LLM.

- A flow begins with one or more inputs, usually a question or prompt entered by a user, 
and in the case of iterative conversations the chat history to this point.

The flow is then defined as a series of connected tools, each of which performs 
a specific operation on the inputs and other environmental variables. 
There are multiple types of tool that you can include in a prompt flow to perform tasks such as:

1- Running custom Python code
2- Looking up data values in an index
3- Creating prompt variants - enabling you to define multiple versions of a prompt for a large language model (LLM), varying system messages or prompt wording, and compare and evaluate the results from each variant.
4- Submitting a prompt to an LLM to generate results.

Finally, the flow has one or more outputs, typically to return the generated results from an LLM.

### Using the RAG pattern in a prompt flow

- The key to using the RAG pattern in a prompt flow is to use an Index Lookup 
tool to retrieve data from an index so that subsequent tools in the flow can 
use the results to augment the prompt used to generate output from an LLM.

### Use a sample to create a chat flow

![alt text](./images-cert/image6.png)

1- Modify query with history:
The first step in the flow is a Large Language Model (LLM) node that takes the chat history and the user's last question 
and generates a new question that includes all necessary information.

2- Look up relevant information:
Next, you use the Index Lookup tool to query the search index you created with the integrated 
Azure AI Search feature and find the relevant information from your data source.

3- Generate prompt context:
The output of the Index Lookup tool is the retrieved context you want to use 
when generating a response to the user. You want to use the output in a prompt 
that is sent to a language model, which means you want to parse the output into a more suitable format.

4- Define prompt variants:
When you construct the prompt you want to send to your language model, 
you can use variants to represent different prompt contents.
When including RAG in your chat flow, your goal is to ground the chatbot's responses. 
Next to retrieving relevant context from your data source, you can also influence the groundedness of the chatbot's response by instructing it to use the context and aim to be factual.

5- Chat with context:
Finally, you use an LLM node to send the prompt to a language model to generate a 
response using the relevant context retrieved from your data source. 
The response from this node is also the output of the entire flow.

* Note: After configuring the sample chat flow to use your indexed data and the language model of your choosing, you can deploy the flow and integrate it with an application to offer users an agentic experience.


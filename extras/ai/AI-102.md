# Notes

AI solutions are built on machine learning models that encapsulate semantic relationships found in huge quantities of data; enabling applications to appear to interpret input in various formats, reason over the input data, and generate appropriate responses and predictions.

* Table Common AI capabilities that developers can integrate into a software application include:
https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/2-what-is-ai


# Type of AI 

## Generative AI

- Generative AI uses language models to respond to natural language prompts, enabling you to build conversational apps and agents that support research, content creation, and task automation 

- The language models used in generative AI solutions can be large language models (LLMs) that have been trained on huge volumes of data and include many millions of parameters; or they can be small language models (SLMs) that are optimized for specific scenarios with lower overhead. 

- Language models commonly respond to text-based prompts with natural language text; though increasingly new multi-modal models are able to handle image or speech prompts and respond by generating text, code, speech, or images.

- Generative AI applications are built on language models. The development process usually starts with an exploration and comparison of available foundation models to find the one that best suits the particular needs of your application. After selecting a suitable model, you deploy it to an endpoint where it can be consumed by a client application or AI agent.

Foundation models, such as the GPT family of models, are state-of-the-art language models designed to understand, generate, and interact with natural language. Some common use cases for models are:

-- Speech-to-text and text-to-speech conversion. For example, generate subtitles for videos.
-- Machine translation. For example, translate text from English to Japanese.
-- Text classification. For example, label an email as spam or not spam.
-- Entity extraction. For example, extract keywords or names from a document.
-- Text summarization. For example, generate a short one-paragraph summary from a multi-page document.
-- Question answering. For example, provide answers to questions like "What is the capital of France?"
-- Reasoning. For example, solve a mathematical problem.


* IMPORTANT:
The latest breakthrough in generative AI models is owed to the development of the Transformer architecture:

- Instead of processing words sequentially, Transformers process 
each word independently and in parallel by using attention.
- Next to the semantic similarity between words, 
Transformers use positional encoding to include the information about the position of a word in a sentence.




# List Azure AI services

A set of out-of-the-box prebuilt APIs and models that you can integrate into your applications. The following table lists some commonly used Azure AI services

* Table of services 
https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/3-azure-ai-services

* Complete list of services
https://learn.microsoft.com/en-us/azure/ai-services/what-are-ai-services#available-azure-ai-services?azure-portal=true


# How provisioned / create azure services ?

- Portal
- BICEP or ARM templates
- Azure command-line interface
- For most medium to large-scale development scenarios it's better 
use concentrate in an Azure AI Foundry project 
enabling centralize access control and cost management

# Regional availability

Some services and models are available in only a subset of Azure regions.


# Single service or multi-service resource ?

## Single Service

- Most Azure AI services, such as Azure AI Vision, Azure AI Language, and so on, 
can be provisioned as standalone resources.

- Standalone Azure AI services often include a free-tier SKU with limited functionality

- Each standalone Azure AI resource provides an endpoint and authorization keys to access it

## Multi-service

- Encapsulates multiple AI services in a single Azure resource

- We have 2 options types for multi-service: Azure AI services and Azure AI Foundry

![alt text](./images-cert/image.png)


# Azure AI services

- Azure AI services uses a single key and endpoint

# Azure AI Foundry

- Resource connections, data, code, and other elements of the AI solution in projects. (centralized)
- Azure AI Foundry portal, a web-based visual interface for working with AI projects. 
- Azure AI Foundry SDK, which you can use to build AI solutions programmatically


## Types of AI Foundry projects

- Foundry projects

-- Associated with an Azure AI Foundry resource in an Azure subscription. 
-- Foundry projects provide support for Azure AI Foundry models (including OpenAI models), 
Azure AI Foundry Agent Service, Azure AI services, and tools for evaluation 
and responsible AI development
-- More suitable for developers who want to manage resources for AI agent 
or chat app development. 


- Hub-based projects

-- Associated with an Azure AI hub resource.
-- Hub-based projects include an Azure AI Foundry resource, 
as well as managed compute, support for Prompt Flow development, and connected 
Azure storage and Azure key vault resources
-- We can also use Azure AI hub resources in both Azure AI Foundry portal 
and Azure Machine learning portal, making it easier to work on collaborative 
projects that involve data scientists and machine learning specialists as 
well as developers and AI software engineers
-- More suitable for enterprise development teams working on complex AI solutions.

# Developer tools and SDKs

- Azure AI Foundry for VS Code extension:
Creating a project.
Selecting and deploying a model.
Testing a model in the playground.
Creating an agent.

- GitHub and GitHub Copilot

- The Azure AI Foundry Models API, which provides an interface for 
working with generative AI model endpoints hosted in Azure AI Foundry.

- The Azure OpenAI in Azure AI Foundry Models API, 
which enables you to build chat applications based on OpenAI models hosted 
in Azure AI Foundry.
* Using API POST like GEMINI for .NET, if there is no proper SDK

* Difference 
* The Model Inference API primarily concerns the consumption of predictions 
from a wide range of foundational models.
* The Foundry Models API focuses on AI agents and also includes Azure OpenAI 
and other compatible models for agent-based applications.

- Azure AI Services SDKs, AI service-specific libraries for 
multiple programming languages and frameworks that enable you to consume 
Azure AI Services resources in your subscription. 

- The Azure AI Foundry SDK, which enables you to write code to connect to 
Azure AI Foundry projects and access resource connections, 
which you can then work with using service-specific SDKs.

- The Azure AI Foundry Agent Service, which is accessed through the 
Azure AI Foundry SDK and can be integrated with frameworks 
like *Semantic Kernel* to build comprehensive AI agent solutions.


# Responsible AI

Probabilistic models, which are in turn dependent on the data with 
which they were trained.

- Fairness, should treat all people fairly. Example, 
suppose you create a machine learning model to support a loan approval 
application for a bank. The model should make predictions of whether or not 
the loan should be approved without incorporating any bias based on gender, ethnicity, 
or other factors that might result in an unfair advantage or disadvantage to specific groups of applicants.

- Reliability and safety, a machine learning model that diagnoses 
patient symptoms and recommends prescriptions. 
Unreliability in these kinds of system can result in substantial 
risk to human life.


- Privacy and security
The machine learning models on which AI systems are based rely on large volumes 
of data, which may contain personal details that must be kept private. 
Even after models are trained and the system is in production, 
they use new data to make predictions or take action that may be subject 
to privacy or security concerns; so appropriate safeguards to protect data 
and customer content must be implemented.

- Inclusiveness
AI systems should empower everyone and engage people. 
AI should bring benefits to all parts of society, 
regardless of physical ability, gender, sexual orientation, ethnicity, 
or other factors.

- Transparency
AI systems should be understandable. Users should be made fully 
aware of the purpose of the system, 
how it works, and what limitations may be expected.

- Accountability
Designers and developers of AI-based solution should work within a framework 
of governance and organizational principles that ensure the solution 
meets responsible and legal standards that are clearly defined.


# Choose and deploy models from the model catalog in Azure AI Foundry portal

# Selecting a foundation model for your generative AI app 

## Can AI solve my use case?

To start answering this question, you need to discover, filter, and deploy a model. 
We can explore the available language models through three different catalogs:

- Hugging Face: Vast catalog of open-source models across various domains.
- GitHub: Access to diverse models via GitHub Marketplace and GitHub Copilot.
- Azure AI Foundry: Comprehensive catalog with robust tools for deployment.

### Choose between large and small language models

- Large Language Models (LLMs) like GPT-4, Mistral Large, Llama3 70B, 
Llama 405B, and Command R+ are powerful AI models 
designed for tasks that require deep reasoning, 
complex content generation, and extensive context understanding.

- Small Language Models (SLMs) like Phi3, Mistral OSS models, and Llama3 8B 
are efficient and cost-effective, while still handling many common 
Natural Language Processing (NLP) tasks. They're perfect for running on 
lower-end hardware or edge devices, where cost and speed are more important than model complexity.


### Focus on a modality, task, or tool

- *chat completion models*: Language models like GPT-4 and Mistral Large are 
also known as designed to generate coherent and contextually appropriate text-based responses.

- *reasoning models*: Tasks like math, coding, science, strategy, and logistics, 
you can use like DeepSeek-R1 and o1.

- *multi-modal*: meaning they can process images, audio, and other data 
types alongside text. Models like GPT-4o and Phi3-vision are capable of analyzing and generating both text and images.

- *generating images*: tools like DALL·E 3 and Stability AI can create realistic visuals from text prompts.

- *embedding models*: like Ada and Cohere. Embeddings models convert 
text into numerical representations and are used to improve search 
relevance by understanding semantic meaning. 
These models are often implemented in *Retrieval Augmented Generation (RAG)* 
scenarios to enhance recommendation engines by linking similar content.

* When you want to build an application that interacts with other software 
tools dynamically, you can add function calling and JSON support. 


### Specialize with regional and domain-specific models

Certain models are designed for specific languages, regions, or industries. 
These models can outperform general-purpose generative AI in their respective domains. 

For example:

- Core42 JAIS is an Arabic language LLM, making it the best 
choice for applications targeting Arabic-speaking users.

- Mistral Large has a strong focus on European languages, 
ensuring better linguistic accuracy for multilingual applications.

- Nixtla TimeGEN-1 specializes in time-series forecasting, making it ideal for financial predictions, supply chain optimization, and demand forecasting.

* If your project has regional, linguistic, or industry-specific needs, these models can provide more relevant results than general-purpose AI.

### Balance flexibility and performance with open versus proprietary models

- *Proprietary models* are best for cutting-edge performance and enterprise use. 
Azure offers models like OpenAI’s GPT-4, Mistral Large, and Cohere Command R+, 
which deliver industry-leading AI capabilities. 
These models are ideal for businesses needing enterprise-level security, support, and high accuracy.

- *Open-source models* are best for flexibility and cost-efficiency. 
Many open-source models available in the Azure AI Foundry model catalog 
from Hugging Face, and models from Meta, Databricks, Snowflake, and Nvidia. 

* All the models available in Azure AI Foundry meets the key enterprise requirements for usage:
- Data and privacy: you get to decide what happens with your data.
- Security and compliance: built-in security.
- Responsible AI and content safety: evaluations and content safety.


## How do I select the best model for my use case?

- Task type: What type of task do you need the model to perform? 
Does it include the understanding of only text, or also audio, or video, or multiple modalities?

- Precision: Is the base model good enough or do you need a fine-tuned model 
that is trained on a specific skill or dataset?

- Openness: Do you want to be able to fine-tune the model yourself?

- Deployment: Do you want to deploy the model locally, 
on a serverless endpoint, or do you want to manage the deployment infrastructure?

### Filter models for precision

- When integrating a language model into an app, you can choose between a 
base model or a fine-tuned model. 
A base model, like GPT-4, is pretrained on a large dataset and can 
handle various tasks but can lack precision for specific domains. 
Techniques like prompt engineering can improve this, but sometimes fine-tuning is necessary.

- A *f*ine-tuned model* is trained further on a smaller, task-specific dataset to improve its precision and ability to generate relevant outputs for specific applications. You can either use a *fine-tuned model* or *fine-tune* a model yourself.

### Filter models for performance

* Table When you're exploring models through the Azure AI Foundry model catalog, 
you can use model benchmarks to compare publicly available metrics:

https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/2-select-model

- To evaluate how a selected model performs regarding your specific requirements, 
you can consider manual or automated evaluations.

- *Manual evaluations* allow you to rate your model's responses. 
- *Automated evaluations* include traditional machine learning metrics and AI-assisted metrics that are calculated and generated for you.


## Can I scale for real-world workloads?

Considerations for scaling a generative AI solution include:

- Model deployment: Where will you deploy the model for the best balance of performance and cost?
- Model monitoring and optimization: How will you monitor, evaluate, and optimize model performance?
- Prompt management: How will you orchestrate and optimize prompts to maximize the accuracy and relevance of generated responses?
- Model lifecycle: How will you manage model, data, and code updates as part of an ongoing Generative AI Operations (GenAIOps) lifecycle?


## Deploy a model to an endpoint

When you develop a generative AI app, you need to integrate language models into your application. 
To be able to use a language model, you need to deploy the model.

1- API request is sent to the endpoint.
2- The endpoint specifies the model that processes the request.
3- The result is sent back to the app through an API response.


When you deploy a language model with Azure AI Foundry, 
you have several types available, which depend on the model you want to deploy.

- Standard deployment: Models are hosted in the Azure AI Foundry project resource.

- Serverless compute: Models are hosted in Microsoft-managed dedicated serverless endpoints in an Azure AI Foundry hub project.

- Managed compute: Models are hosted in managed virtual machine images in an Azure AI Foundry hub project.


* The associated cost depends on the type of model you deploy, 
which deployment option you choose, and what you are doing with the model:
* Standard deployment is recommended for most scenarios.

![alt text](./images-cert/image2.png)


# Apply prompt patterns to optimize your model's output

- The process of designing and optimizing prompts to improve the model's performance 
is also known as *prompt engineering*

- An effective approach is to add instructions to your deployed model 
in the form of a system prompt. The system prompt sets the model's behavior 
and allows you to guide the model without exposing the end user to the 
instructions. 
The best results are often achieved through the assignment of an *explicit system prompt* and *guidance (or templates)*

## Specify the desired format for responses

- When you want the model to generate output in a specific format, 
you can provide a template or structure in your prompt.
Example, if you're a sports reporting composing a historical article, 
you can request that the model follow a specific template,
which includes headings, bullet points, and data breakdowns.

## Add context

- When you want the model to focus on specific topics, 
you can specify the context to consider.

No context specified:
When should I visit Edinburgh?

With context specified:
When should I visit Edinburgh?
I'm particularly interested in attending Scotland's home matches 
in the Six Nations rugby tournament.


## Apply model optimization strategies

- *Retrieval Augmented Generation (RAG)*: A technique that involves 
using a data source to provide grounding context to prompts. 
RAG can be a useful approach when you need the model to answer questions 
based on a specific knowledge domain or when you need the model to consider 
information related to events that occurred after the training data on which the model is based.

- *Fine-tuning*: A technique that involves extending the training of a 
foundation model by providing example prompts and responses that reflect 
the desired output format and style.

## The strategy you should choose as a developer depends requirements

1- Prompt engineering
2- Combined strategies
3- Retrieval Augmented Generation (RAG)
4- Fine-tuning

- Optimize for context: When the model lacks contextual knowledge 
and you want to maximize responses accuracy: we can us RAG for example

- Optimize the model: When you want to improve the response format, style, 
or speech by maximizing consistency of behavior: With fine-tuning, we train a 
base language model on a dataset of example prompts and responses before 
integrating it in your application, with the result that the fine-tuned model 
will produce responses that are consistent with the examples 
in the fine-tuning training dataset.

* We can use any combination of optimization strategies, 
for example prompt engineering, RAG and a fine-tuned model, 
to improve your language application.



# Get started with prompt flow to develop language model apps in the Azure AI Foundry

## Prompt flow

- To harness the power of the LLMs available, you need to create an application that combines your data sources with LLMs and generates the desired output.

- To develop, test, tune, and deploy LLM applications, you can use prompt flow, accessible in the *Azure Machine Learning studio* and 
the *Azure AI Foundry portal*.

### Understand the development lifecycle of a large language model (LLM) app

![alt text](./images-cert/image3.png)

1- Initialization: Define the use case and design the solution.
2- Experimentation: Develop a flow and test with a small dataset.
3- Evaluation and refinement: Assess the flow with a larger dataset.
4- Production: Deploy and monitor the flow and application.


#### Initialization

1- Define the objective
2- Collect a sample dataset
3- Build a basic prompt
4- Design the flow

#### Experimentation

- The experimentation phase is an iterative process during which you 
(1) run the flow against a sample dataset. You then 
(2) evaluate the prompt's performance. If you're 
(3) satisfied with the result, you can move on to evaluation and refinement. If you think there's room for improvement, you can 
(4) modify the flow by changing the prompt or flow itself.

#### Evaluation and refinement

- When you're satisfied with the output of the flow that classifies news articles, based on the sample dataset, you can assess the flow's performance against a larger dataset.

#### Production

During production, you:

1- Optimize the flow that classifies/process/etc
2- Deploy your flow to an endpoint. When you call the endpoint, the flow is triggered to run and the desired output is generated.
3- Monitor the performance of your solution by collecting usage data and end-user feedback. 
By understanding how the application performs, you can improve the flow whenever necessary.


## Understand prompt flow's core components

- Prompt flow is a feature within Azure AI Foundry that allows you to author flows. Flows are executable workflows often consist of three parts:

1- Inputs: Represent data passed into the flow. Can be different data types like strings, integers, or boolean.
2- Nodes: Represent tools that perform data processing, task execution, or algorithmic operations.
3- Outputs: Represent the data produced by the flow.

- Similar to a pipeline, a flow can consist of multiple nodes that 
can use the flow's inputs or any output generated by another node. 
You can add a node to a flow by choosing one of the available types of tools.

### Tools available in prompt flow

Three common tools are:

- LLM tool: Enables custom prompt creation utilizing Large Language Models.
- Python tool: Allows the execution of custom Python scripts.
- Prompt tool: Prepares prompts as strings for complex scenarios or integration with other tools.

* Note: If we're looking for functionality that is not offered by the available tools, we can create your own custom tool

By defining the inputs, connecting nodes, and defining the desired outputs, you can create a flow. Flows help you create LLM applications for various purposes.

### Understand the types of flows

There are three different types of flows you can create with prompt flow:

- Standard flow: Ideal for general LLM-based application development, offering a range of versatile tools.
- Chat flow: Designed for conversational applications, with enhanced support for chat-related functionalities.
- Evaluation flow: Focused on performance evaluation, allowing the analysis and improvement of models or 
applications through feedback on previous runs.


## Explore connections and runtimes


When we create a Large Language Model (LLM) application with prompt flow, 
we first need to configure any necessary *connections* and *runtimes*

### Connections

- Whenever we want our flow to connect to external data source, service, or API,
we need our flow to be authorized to communicate with that external service. 

When we create a connection, we configure a secure link between prompt flow and 
external services

- Depending on the type of connection we create, 
the connection securely stores the endpoint, API key, or 
credentials necessary for prompt flow to communicate with the external service. 
Any necessary secrets aren't exposed to users, but instead are stored in an *Azure Key Vault*

By setting up connections, users can easily reuse external services necessary for tools in their flows.

* Certain built-in tools require you to have a connection configured:

- Summary:

1- Connections automate API credential management
2- Connections enable secure data transfer from various sources, crucial for 
maintaining data integrity and privacy across different environments.

### Runtimes

Are a combination of a compute instance:
1- They provide necessary compute resources
2- Thet specify the necessary packages and libraries that need to be installed before being able to run the flow.

When you use runtimes, you have a controlled environment where flows can be run and validated, ensuring that everything works as intended in a stable setting.

* Note: A default environment is available for quick development and testing. 
When we require other packages to be installed, you can create a *custom environment*


## Explore variants and monitoring options


### Variants

- Prompt flow variants are versions of a tool node with distinct settings. 

- Currently, variants are only supported in the LLM tool

- Variant can represent a different prompt content or connection setting. 
Variants allow users to customize their approach for specific tasks, like, summarizing news articles.

Benefits:

- Enhance the quality of your LLM generation
- Save time and effort
- Boost productivity
- Facilitate easy comparison


### Deploy flow to an endpoint

- When we're satisfied with the performance of your flow, we can choose to deploy it to an online endpoint

### Metrics 

To understand whether your application is meeting practical needs:

- We can collect end-user feedback and assess the application's usefulness. 

- Another approach to understanding whether your application is performing well, 
is by comparing LLM predictions with expected or ground truth responses to gauge accuracy and relevance.

- Groundedness: Measures alignment of the LLM application's output with the input source or database.
- Relevance: Assesses how pertinent the LLM application's output is to the given input.
- Coherence: Evaluates the logical flow and readability of the LLM application's text.
- Fluency: Assesses the grammatical and linguistic accuracy of the LLM application's output.
- Similarity: Quantifies the contextual and semantic match between the LLM application's output and the ground truth.

Whenever our LLM application doesn't perform as expected, 
we need to revert back to experimentation to iteratively explore how to improve the flow


# Develop a RAG-based solution with your own data using Azure AI Foundry

## Ungrounded prompts and responses


